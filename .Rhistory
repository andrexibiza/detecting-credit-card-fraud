extreme_proportions <- (simulated_proportions <= observed_proportion) |
(simulated_proportions >= (1 - observed_proportion + p0))
p_value <- sum(extreme_proportions) / num_simulations
p_value
# Plot the histogram of simulated proportions
hist(simulated_proportions, breaks = 40, main = "Histogram of Simulated Proportions",
xlab = "Proportion", ylab = "Frequency", xlim = c(0.4, 1.0), col = "blue")
# Add a vertical line for the observed proportion
abline(v = observed_proportion, col = "red", lwd = 2)
# Add a legend
legend("topright", legend = c("Observed Proportion", "Simulated Proportions"),
fill = c("red", "blue"))
# Plot the histogram of simulated proportions
hist(simulated_proportions, breaks = 40, main = "Histogram of Simulated Proportions",
xlab = "Proportion", ylab = "Frequency", xlim = c(0.4, 1.0), col = "blue")
# Add a vertical line for the observed proportion
abline(v = observed_proportion, col = "red", lwd = 2)
# Add a legend
legend("topright", legend = c("Observed-p", "Simulated-p"),
fill = c("red", "blue"))
# Plot the histogram of simulated proportions
hist(simulated_proportions, breaks = 20, main = "Histogram of Simulated Proportions",
xlab = "Proportion", ylab = "Frequency", xlim = c(0.4, 1.0), col = "blue")
# Add a vertical line for the observed proportion
abline(v = observed_proportion, col = "red", lwd = 2)
# Add a legend
legend("topright", legend = c("Observed-p", "Simulated-p"),
fill = c("red", "blue"))
# Plot the histogram of simulated proportions
hist(simulated_proportions, breaks = 30, main = "Histogram of Simulated Proportions",
xlab = "Proportion", ylab = "Frequency", xlim = c(0.4, 1.0), col = "blue")
# Add a vertical line for the observed proportion
abline(v = observed_proportion, col = "red", lwd = 2)
# Add a legend
legend("topright", legend = c("Observed-p", "Simulated-p"),
fill = c("red", "blue"))
# Plot the histogram of simulated proportions
hist(simulated_proportions, breaks = 20, main = "Histogram of Simulated Proportions",
xlab = "Proportion", ylab = "Frequency", xlim = c(0.4, 1.0), col = "blue")
# Add a vertical line for the observed proportion
abline(v = observed_proportion, col = "red", lwd = 2)
# Add a legend
legend("topright", legend = c("Observed-p", "Simulated-p"),
fill = c("red", "blue"))
# Simulation parameters
n <- 30                    # Sample size
p0 <- 0.69                 # Null hypothesis success probability
num_simulations <- 10000   # Number of simulations
# Perform simulations
set.seed(123)  # For reproducibility
simulated_proportions <- rbinom(num_simulations, n, p0) / n
# Calculate the p-value
observed_proportion <- 17 / 30
extreme_proportions <- (simulated_proportions <= observed_proportion) |
(simulated_proportions >= (1 - observed_proportion + p0))
p_value <- sum(extreme_proportions) / num_simulations
p_value
# Plot the histogram of simulated proportions
hist(simulated_proportions, breaks = 40, main = "Histogram of Simulated Proportions",
xlab = "Proportion", ylab = "Frequency", xlim = c(0.4, 1.0), col = "blue")
# Add a vertical line for the observed proportion
abline(v = observed_proportion, col = "red", lwd = 2)
# Add a legend
legend("topright", legend = c("Observed", "Simulated"),
fill = c("red", "blue"))
# Plot the histogram of simulated proportions
hist(simulated_proportions, breaks = 25, main = "Histogram of Simulated Proportions",
xlab = "Proportion", ylab = "Frequency", xlim = c(0.4, 1.0), col = "blue")
# Add a vertical line for the observed proportion
abline(v = observed_proportion, col = "red", lwd = 2)
# Add a legend
legend("topright", legend = c("Observed", "Simulated"),
fill = c("red", "blue"))
# Plot the histogram of simulated proportions
hist(simulated_proportions, breaks = 20, main = "Histogram of Simulated Proportions",
xlab = "Proportion", ylab = "Frequency", xlim = c(0.4, 1.0), col = "blue")
# Add a vertical line for the observed proportion
abline(v = observed_proportion, col = "red", lwd = 2)
# Add a legend
legend("topright", legend = c("Observed", "Simulated"),
fill = c("red", "blue"))
# Plot the histogram of simulated proportions
hist(simulated_proportions, breaks = 40, main = "Histogram of Simulated Proportions",
xlab = "Proportion", ylab = "Frequency", xlim = c(0.4, 1.0), col = "blue")
# Add a vertical line for the observed proportion
abline(v = observed_proportion, col = "red", lwd = 2)
# Add a legend
legend("topright", legend = c("Observed", "Simulated"),
fill = c("red", "blue"))
# Calculate the observed proportions
p_hat_prov <- 5 / 20
p_hat_con <- 15 / 25
# Calculate the observed difference in intervention rates
obs_diff <- p_hat_prov - p_hat_con
obs_diff
# Define the sample sizes and observed successes
n_prov <- 20
n_con <- 25
success_prov <- 5
success_con <- 15
# Calculate the observed proportions
p_hat_prov <- success_prov / n_prov
p_hat_con <- success_con / n_con
# Calculate the observed difference in proportions
obs_diff <- p_hat_prov - p_hat_con
# Define the number of simulations
num_simulations <- 10000
# Perform the simulations
set.seed(123) # For reproducibility
sim_diffs <- replicate(num_simulations, {
sim_prov <- rbinom(1, n_prov, p_hat_prov)
sim_con <- rbinom(1, n_con, p_hat_con)
(sim_prov / n_prov) - (sim_con / n_con)
})
# Calculate the p-value based on the simulations
p_value <- mean(abs(sim_diffs) >= abs(obs_diff))
p_value
# Load packages
library(tidyverse)
library(stats)
# Chi square goodness of fit test
# to test the distribution of species in the penguins dataset
# Observe actual frequencies for each species
actual_frequencies <- table(penguins$species)
setwd("//wdmycloudex4100/Axel-Ibiza/Andrex_Ibiza/DATA5100_Statistical-Methods/Final Project - PalmerPenguins")
# Load packages
library(tidyverse)
library(stats)
# Load Week 12 data, verify structure
df <- readRDS("penguins_clean_w12.rds")
str(df)
# Chi square goodness of fit test
# to test the distribution of species in the penguins dataset
# Observe actual frequencies for each species
actual_frequencies <- table(df$species)
# Assuming we expect the species to be evenly distributed
expected_frequencies <- rep(1/length(unique(df$species)), length(unique(df$species)))
# Perform chi-square goodness of fit test
chi_square_test <- chisq.test(x = actual_frequencies, p = expected_frequencies)
# Output the result of the chi-square test
chi_square_test
# Load packages
library(tidyverse)
library(stats)
# Load Week 12 data, verify structure
df <- readRDS("penguins_clean_w12.rds")
str(df)
# Chi square goodness of fit test
# to test the distribution of species in the penguins dataset
# Set seed for reproducibility
set.seed(666)
# Observe actual frequencies for each species
actual_frequencies <- table(df$species)
# Assuming we expect the species to be evenly distributed
expected_frequencies <- rep(1/length(unique(df$species)), length(unique(df$species)))
# Perform chi-square goodness of fit test
chi_square_test <- chisq.test(x = actual_frequencies, p = expected_frequencies)
# Output the result of the chi-square test
chi_square_test
# Load packages
library(tidyverse)
library(stats)
# Load Week 12 data, verify structure
df <- readRDS("penguins_clean_w12.rds")
str(df)
# Chi square goodness of fit test
# to test the distribution of species in the penguins dataset
# Set seed for reproducibility
set.seed(666)
# Observe actual frequencies for each species
actual_frequencies <- table(df$species)
# Assuming we expect the species to be evenly distributed
expected_frequencies <- rep(1/length(unique(df$species)), length(unique(df$species)))
# Perform chi-square goodness of fit test
chi_square_test <- chisq.test(x = actual_frequencies, p = expected_frequencies)
# Output the result of the chi-square test
chi_square_test
# Load packages
library(tidyverse)
library(stats)
# Load Week 12 data, verify structure
df <- readRDS("penguins_clean_w12.rds")
str(df)
# Chi square goodness of fit test
# to test the distribution of species in the penguins dataset
# Set seed for reproducibility
set.seed(666)
# Observe actual frequencies for each species
actual_frequencies <- table(df$species)
# Assuming we expect the species to be evenly distributed
expected_frequencies <- rep(1/length(unique(df$species)), length(unique(df$species)))
# Perform chi-square goodness of fit test
chi_square_test <- chisq.test(x = actual_frequencies, p = expected_frequencies)
# Output the result of the chi-square test
chi_square_test
##### Chi-square test for independence
### between `species` and `island`
# Test the independence between 'species' and 'island' in the penguins dataset
# Create a contingency table of the two categorical variables
contingency_table <- table(df$species, df$island)
# Perform chi-square test of independence
chi_square_independence_test <- chisq.test(contingency_table)
# Output the result of the chi-square test of independence
chi_square_independence_test
##### Chi-square test for independence
### between `species` and `island`
# Test the independence between 'species' and 'island' in the penguins dataset
# Create a contingency table of the two categorical variables
contingency_table <- table(df$species, df$island)
# Perform chi-square test of independence
chi_square_independence_test <- chisq.test(contingency_table)
# Output the result of the chi-square test of independence
chi_square_independence_test
# Load packages
library(tidyverse)
library(stats)
# Load Week 12 data, verify structure
df <- readRDS("penguins_clean_w12.rds")
str(df)
###### Chi square goodness of fit test
### to test the distribution of species in the penguins dataset
# Set seed for reproducibility
set.seed(666)
# Observe actual frequencies for each species
actual_frequencies <- table(df$species)
# Assuming we expect the species to be evenly distributed
expected_frequencies <- rep(1/length(unique(df$species)), length(unique(df$species)))
# Perform chi-square goodness of fit test
chi_square_test <- chisq.test(x = actual_frequencies, p = expected_frequencies)
# Output the result of the chi-square test
chi_square_test
# Chi-squared test for given probabilities
#
# data:  actual_frequencies
# X-squared = 31.907, df = 2, p-value = 1.179e-07
##### Chi-square test for independence
### between `species` and `island`
# Test the independence between 'species' and 'island' in the penguins dataset
# Create a contingency table of the two categorical variables
contingency_table <- table(df$species, df$island)
# Perform chi-square test of independence
chi_square_independence_test <- chisq.test(contingency_table)
# Output the result of the chi-square test of independence
chi_square_independence_test
# Pearson's Chi-squared test
#
# data:  contingency_table
# X-squared = 299.55, df = 4, p-value < 2.2e-16
# No dataset changes this week
write.csv(df, "penguins_clean_w13.csv") #does not preserve structure of variables
write_rds(df, "penguins_clean_w13.rds") #preserves structure of variables
# Given values
r <- 0.636  # Correlation coefficient
mean_distance <- 108  # Mean distance in miles
sd_distance <- 99  # Standard deviation of distance in miles
mean_travel_time <- 129  # Mean travel time in minutes
sd_travel_time <- 113  # Standard deviation of travel time in minutes
# Calculate the slope (m)
slope <- r * (sd_travel_time / sd_distance)
# Calculate the intercept (b)
intercept <- mean_travel_time - (slope * mean_distance)
# Calculate R^2
r_squared <- r^2
# Predict travel time for the distance between Santa Barbara and Los Angeles
santa_barbara_los_angeles_distance <- 103  # Distance in miles
predicted_travel_time <- (slope * santa_barbara_los_angeles_distance) + intercept
# Actual travel time and residual calculation
actual_travel_time <- 168  # Actual travel time in minutes
residual <- actual_travel_time - predicted_travel_time
# Output the regression equation, R^2, predicted travel time, and residual
list(
regression_equation = paste("y =", slope, "* x +", intercept),
R_squared = r_squared,
predicted_travel_time = predicted_travel_time,
residual = residual
)
# Given values
r <- 0.636  # Correlation coefficient
mean_distance <- 108  # Mean distance in miles
sd_distance <- 99  # Standard deviation of distance in miles
mean_travel_time <- 129  # Mean travel time in minutes
sd_travel_time <- 113  # Standard deviation of travel time in minutes
# Calculate the slope (m)
slope <- r * (sd_travel_time / sd_distance)
# Calculate the intercept (b)
intercept <- mean_travel_time - (slope * mean_distance)
# Calculate R^2
r_squared <- r^2
# Predict travel time for the distance between Santa Barbara and Los Angeles
santa_barbara_los_angeles_distance <- 103  # Distance in miles
predicted_travel_time <- (slope * santa_barbara_los_angeles_distance) + intercept
# Actual travel time and residual calculation
actual_travel_time <- 168  # Actual travel time in minutes
residual <- actual_travel_time - predicted_travel_time
# Output the regression equation, R^2, predicted travel time, and residual
list(
regression_equation = paste("y =", slope, "* x +", intercept),
R_squared = r_squared,
predicted_travel_time = predicted_travel_time,
residual = residual
)
# Given values
slope_estimate <- 2.559
slope_stderr <- 0.390
sample_size <- 20
confidence_level <- 0.95
# Degrees of freedom for a two-sided t-test
df <- sample_size - 2
# t-critical value for 95% confidence interval
t_critical <- qt((1 + confidence_level) / 2, df)
# Confidence interval calculation
lower_bound <- slope_estimate - (t_critical * slope_stderr)
upper_bound <- slope_estimate + (t_critical * slope_stderr)
# Output the results
cat("The 95% confidence interval for the slope is (", lower_bound, ", ", upper_bound, ")\n")
cat("The t-critical value used for the calculation is ", t_critical, "\n")
![CCfraudimage](CC-security.jpg)
# Install packages
install.packages("psych")
install.packages("naniar")
install.packages("e1071")
install.packages("randomForest")
install.packages("caret")
# Load packages
library(tidyverse)
library(e1071)
library(caret)
# Load data
df = read_csv('credit_card_fraud.csv', show_col_types = FALSE)
# Examine structure
str(df)
install.packages(Rtools)
# Data preprocessing
# Convert appropriate columns to factors
df$merchant <- as.factor(df$merchant)
df$category <- as.factor(df$category)
df$city <- as.factor(df$city)
df$state <- as.factor(df$state)
df$job <- as.factor(df$job)
df$is_fraud <- as.factor(df$is_fraud)
# Verify changes
str(df)
# Selecting a subset of data for demonstration, if necessary
set.seed(666) # For reproducibility
data_subset <- df[sample(nrow(df), 10000), ] # Corrected 'data' to 'df'
# Encoding categorical variables using one-hot encoding
data_processed <- data_subset %>%
mutate(across(c(merchant, category), ~as.factor(.)))
# Checking for missing values
sum(is.na(data_processed))
# Scaling continuous variables (example)
data_processed$amt <- scale(data_processed$amt)
# Splitting data into training and testing sets
set.seed(666)
training_index <- sample(1:nrow(data_processed), 0.8 * nrow(data_processed))
train <- data_processed[training_index, ]
test <- data_processed[-training_index, ]
# Naive Bayes model
model <- naiveBayes(is_fraud ~ ., data = train)
summary(model)
# Predicting
predictions <- predict(model, test)
# Evaluating the model
table(predictions, test$is_fraud)
# Create a confusion matrix
conf_matrix <- confusionMatrix(data = predictions, reference = test$is_fraud)
# Extract precision, recall, and F1-score from the confusion matrix
precision <- conf_matrix$byClass['Pos Pred Value']
recall <- conf_matrix$byClass['Sensitivity']
f1_score <- conf_matrix$byClass['F1']
# Print the performance metrics
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
# Set seed for reproducibility
set.seed(666)
# Define Laplace smoothing factor (adjust as needed)
laplace <- 2
# Build Naive Bayes model with Laplace smoothing
model_laplace <- naiveBayes(is_fraud ~ ., data = train, laplace = laplace)
# Summarize the model
summary(model_laplace)
# Make predictions on the test set
predictions_laplace <- predict(model_laplace, test)
# Evaluate the model
table(predictions_laplace, test$is_fraud)
# Create a confusion matrix for the model with Laplace smoothing
conf_matrix_laplace <- confusionMatrix(data = predictions_laplace, reference = test$is_fraud)
# Extract precision, recall, and F1-score
precision_laplace <- conf_matrix_laplace$byClass['Pos Pred Value']
recall_laplace <- conf_matrix_laplace$byClass['Sensitivity']
f1_score_laplace <- conf_matrix_laplace$byClass['F1']
# Print the performance metrics for the model with Laplace smoothing
cat("Performance with Laplace Smoothing:\n")
cat("Precision:", precision_laplace, "\n")
cat("Recall:", recall_laplace, "\n")
cat("F1-score:", f1_score_laplace, "\n")
# Set seed for reproducibility
set.seed(666)
# Define the number of folds for cross-validation
k <- 10
# Create k-fold cross-validation folds
folds <- createFolds(data_processed$is_fraud, k = k, list = FALSE)
# Initialize vectors to store performance metrics
cv_accuracy <- vector(length = k)
cv_precision <- vector(length = k)
cv_recall <- vector(length = k)
cv_f1 <- vector(length = k)
# Loop through each fold
for (i in 1:k) {
# Define training and testing sets for the current fold
test_index <- which(folds == i)
train_data <- data_processed[-test_index, ]
test_data <- data_processed[test_index, ]
# Build Naive Bayes model (add Laplace smoothing if desired)
model <- naiveBayes(is_fraud ~ ., data = train_data)
# Make predictions on the test fold
predictions <- predict(model, test_data)
# Calculate and store performance metrics for the current fold
conf_matrix <- confusionMatrix(predictions, test_data$is_fraud)
cv_accuracy[i] <- conf_matrix$overall['Accuracy']
cv_precision[i] <- conf_matrix$byClass['Pos Pred Value']
cv_recall[i] <- conf_matrix$byClass['Sensitivity']
cv_f1[i] <- conf_matrix$byClass['F1']
}
# Print individual fold metrics
cat("Fold Accuracies:", cv_accuracy, "\n")
cat("Fold Precisions:", cv_precision, "\n")
cat("Fold Recalls:", cv_recall, "\n")
cat("Fold F1-scores:", cv_f1, "\n")
# Calculate and print average performance metrics
cat("Average Accuracy:", mean(cv_accuracy), "\n")
cat("Average Precision:", mean(cv_precision), "\n")
cat("Average Recall:", mean(cv_recall), "\n")
cat("Average F1-score:", mean(cv_f1), "\n")
# Set seed for reproducibility
set.seed(666)
# Define the number of folds for cross-validation
k <- 10
# Create k-fold cross-validation folds
folds <- createFolds(data_processed$is_fraud, k = k, list = FALSE)
# Initialize vectors to store performance metrics
cv_accuracy <- vector(length = k)
cv_precision <- vector(length = k)
cv_recall <- vector(length = k)
cv_f1 <- vector(length = k)
# Loop through each fold
for (i in 1:k) {
# Define training and testing sets for the current fold
test_index <- which(folds == i)
train_data <- data_processed[-test_index, ]
test_data <- data_processed[test_index, ]
# Build Naive Bayes model (add Laplace smoothing if desired)
model <- naiveBayes(is_fraud ~ ., data = train_data, laplace = 2)
# Make predictions on the test fold
predictions <- predict(model, test_data)
# Calculate and store performance metrics for the current fold
conf_matrix <- confusionMatrix(predictions, test_data$is_fraud)
cv_accuracy[i] <- conf_matrix$overall['Accuracy']
cv_precision[i] <- conf_matrix$byClass['Pos Pred Value']
cv_recall[i] <- conf_matrix$byClass['Sensitivity']
cv_f1[i] <- conf_matrix$byClass['F1']
}
# Print individual fold metrics
cat("Fold Accuracies:", cv_accuracy, "\n")
cat("Fold Precisions:", cv_precision, "\n")
cat("Fold Recalls:", cv_recall, "\n")
cat("Fold F1-scores:", cv_f1, "\n")
# Calculate and print average performance metrics
cat("Average Accuracy:", mean(cv_accuracy), "\n")
cat("Average Precision:", mean(cv_precision), "\n")
cat("Average Recall:", mean(cv_recall), "\n")
cat("Average F1-score:", mean(cv_f1), "\n")
# Load necessary libraries
library(randomForest)
library(caret) # for creating training and test datasets and preprocessing
library(e1071) # might be needed for some metric calculations
# Convert categorical variables to factors
training_data <- data_subset
training_data$is_fraud <- as.factor(training_data$is_fraud)
# Splitting data into training and testing sets
set.seed(456) # Seed for reproducibility
training_rows <- createDataPartition(training_data$is_fraud, p = 0.8, list = FALSE)
training_data <- training_data[training_rows, ]
testing_data <- training_data[-training_rows, ]
# Exclude variables with too many categories
exclude_vars <- sapply(training_data, function(x) is.factor(x) && length(levels(x)) > 53)
training_data <- training_data[, !exclude_vars]
testing_data <- testing_data[, !exclude_vars]
# Training a Random Forest model
set.seed(789) # Seed for reproducibility
rf_model <- randomForest(is_fraud ~ ., data = training_data, ntree = 500, mtry = sqrt(ncol(training_data)-1))
# Making predictions
predictions <- predict(rf_model, testing_data)
# Create confusion matrix
conf_matrix <- confusionMatrix(predictions, testing_data$is_fraud)
# Extract performance metrics
accuracy <- conf_matrix$overall['Accuracy']
precision <- conf_matrix$byClass['Pos Pred Value']
recall <- conf_matrix$byClass['Sensitivity']
f1_score <- conf_matrix$byClass['F1']
# Print performance metrics
cat("Performance Metrics:\n")
cat("Accuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-score:", f1_score, "\n")
install.packages("tinytex")
source("//wdmycloudex4100/Axel-Ibiza/Andrex_Ibiza/DATA5100_Statistical-Methods/Data Mining/Credit-Card-Fraud/credit-card-fraud.rmd")
source("//wdmycloudex4100/Axel-Ibiza/Andrex_Ibiza/DATA5100_Statistical-Methods/Data Mining/Credit-Card-Fraud/credit-card-fraud.rmd")
source("//wdmycloudex4100/Axel-Ibiza/Andrex_Ibiza/DATA5100_Statistical-Methods/Data Mining/Credit-Card-Fraud/credit-card-fraud.rmd")
source("//wdmycloudex4100/Axel-Ibiza/Andrex_Ibiza/DATA5100_Statistical-Methods/Data Mining/Credit-Card-Fraud/credit-card-fraud.rmd")
